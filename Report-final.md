# Report-Midterm

### 0. Abstract

With the development of software engineering and the rapid expansion of the scale of software, developers need to understand the runtime details of the program more urgently. A large amount of information is generated during the running of software, including function call information, running time information and other statistical information. By means of visualization, we can show the above information in a more vivid way. Provides a faster understanding of how programs run.

This project implements a statistical performance analysis analyzer based on cProfile and a deterministic performance analysis analyzer based on sys.setprofile, and the above analyzers are encapsulated by python package. After obtaining the program performance data, the browser tool chain is used to visualize the obtained data. Compared with the existing visualization tools, this tool has richer functions and more interactive interface, and combines two performance analysis methods to make the results more reliable.

### Keywords

Performance analysis; Visualization; Software engineering; Visual analysis; Call stack



### 1. Introduction

#### 1.1 Research Background and Significance

With the rapid development of computer science and software engineering, software engineering projects become more and more complex. This complexity manifests itself in many ways: the need for scalability, the complexity of dependencies, collaboration across teams and geographies, and so on. This complexity also brings many challenges, such as the performance of the software becomes more and more difficult to optimize, the internal running logic of the program becomes more and more difficult to understand, and the maintainability and security challenges come with it.

As an important means of software analysis, performance analysis has a very important position. Performance analysis includes deterministic performance analysis and statistical performance analysis. Deterministic performance analysis involves code-level performance analysis and system-level monitoring, which can accurately record all the details of function execution. Statistical performance analysis attempts to analyze code through sampling and probabilistic methods. Due to its small performance overhead and wide application scope, It is widely used for monitoring in production environments. In 2024, a security vulnerability of xz utils was exposed. xz utils 5.6.0 and 5.6.1 versions were backdoor implanted into several stable Linux distributions including Debian, Fedora, etc. Fortunately, a security researcher at Microsoft called Andres Freund, showed the need for profiling tools when he spotted runtime anomalies and reported this suspicious conditions.

#### 1.2 Overview of Visual Performance Analysis and Related Works

Performance analysis generates a large amount of data that is a natural fit for visual analysis. Using graphs and charts to present performance data can help developers and analysts more intuitively understand the performance of the software. The call stack generated by the function execution can be combined with the timeline to draw precise call details, and other execution data can also be used to obtain useful information through statistics and data mining and display by means of charts.

There are a variety of visual performance analysis tools for different platforms and programming languages. Among them, there are not only industrial-grade system monitoring complete solutions, automated testing and monitoring platforms, but also small tools for individual developers. The following sections provide a brief overview of various small performance profiling tools for individual developers, analyze their functionality.

1.   vprof

     vprof is a visual performance analysis tool for Python. It has a built-in web server, which can be used from the command line to visually display CPU usage, memory usage and code internal running data.

2.   gprof2dot

     gprof2dot is an open source script implemented in Python, which is used to convert performance data generated by various performance analysis tools into a visual call graph. The nodes in the call graph are in the structure of a directed acyclic graph, and can display the call relationship between functions and the proportion of execution time of each function.

3.   JetBrain Profiler

     JetBrain IDE provides a convenient performance analysis tool for developers. These tools can help developers to easily detect the CPU and memory usage in the program during the development process, and provide a chart to facilitate developers to view the call relationship between functions.

#### 1.3 Main Content and Organization Structure of the Project

This project takes Python to determine the performance analysis as the starting point, studies the underlying implementation and optimization of the visualization system, the implementation and encapsulation of the analyzer, and completes a full-process Python performance visual analysis system. The specific contents of each chapter are as follows.

This project takes Python to determine the performance analysis as the starting point, studies the underlying implementation and optimization of the visualization system, the implementation and encapsulation of the analyzer, and completes a full-process Python performance visual analysis system. The specific contents of each chapter are as follows.

The 1st chapter is the introduction part, which mainly introduces the research background, research significance, related research work at home and abroad and the overall structure of the paper.

The 2nd chapter is the research of frontend visualization technology, mainly introduces the current browser-based mainstream drawing technology, the comparison of frontend commonly used visualization libraries and the introduction of other libraries, selects the appropriate technical scheme for the visualization of performance data of this project, and describes the overall process of visualization.

The 3rd chapter is the implementation and encapsulation of the profiler, mainly introduces the function and overall architecture of the analyzer and the implementation of RESTFul server.

The 4th chapter is a detailed introduction to the front end of the visualization system, from the combined use of class libraries, interface performance optimization, layout control, and the implementation of custom components.



### 2. Visualization Technology

#### 2.1 Basic Flow of Visualization

The basic flow of the visualization is shown in Figure 2-1.

1. The user selects the Python source file for performance analysis through the interface and uploads it.
2. RESTFul server receives the file uploaded by the frontend and invokes the analyzer module.
3. The profiler module runs the code, analyzes and aggregates the execution data.
4. Pass back the processed data through the RESTFul server.
5. The frontend receives the data and draws the visual interface.

==Figure 2-1==

#### 2.2 Overview of Browser-based Visualization Technology

Browser technology is one of the cornerstones of modern Internet and digital communication. With the development of browser technology, various specifications and tool chains tend to mature. As the carrier of visualization technology, the importance of charts is self-evident. In the early days, drawing in the browser usually relied on plugins such as Flash and Silverlight. These toolchains were gradually replaced by HTML toolchains due to their security, compatibility, and performance advantages. At present, the mainstream drawing technology of the browser is SVG, Canvas, WebGL and so on.

SVG (Scalable Vector Graph) became a W3C recommendation standard in 2003. SVG graphics are vector graphics, which can guarantee no distortion when scaled, and can be used for high-precision graphics. SVG has good compatibility and fully supports DOM. HTML5 Canvas is also one of the W3C standards. Unlike SVG's standard, Canvas's drawing is based on pixels, which will cause aliasing when scaled. Canvas has high drawing efficiency, and it has better performance in drawing with a large amount of data and fragmentary and heavy images.

|                 |                            canvas                            |                             SVG                              |
| --------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Ways to Render  |                       Based on pixels.                       |                        Base on shape.                        |
| Imaging Scaling |                    Zigzag on image edges.                    |                          No zigzag.                          |
| Control Methods |                     Via script language.                     |                 Via script language or CSS.                  |
| Minimal Unit    | The entire canvas is part of the DOM, base graph is manipulated by script. | Every base graph is part of the DOM and can be manipulated independently. |
| Efficiency      |       Efficient, especially when elements are complex.       |                     Not very efficient.                      |
| 3D Support      |              Support 3D by specifing Canvas3D.               |                       Do not support.                        |

==Chart 2-2==

Visual charts contain a large number of basic graphs. As the underlying browser drawing technologies, SVG and Canvas are not usually used directly to visualize the drawing of charts. This task is usually performed by various visualization frameworks. Visualization framework is the encapsulation of SVG and Canvas. Different from SVG and Canvas, which are based on drawing a single graph, all kinds of visualization frameworks provide the encapsulation of various levels of logic/interaction/information display from the perspective of the displayed data, the relationship between graphs and the significance of images. D3, Highcharts, antv.X6 and others are based on SVG implementation, and ECharts and others are based on Canvas implementation.

#### 2.3 Hierarchy of Visualization

As an intuitive way to display data, visual charts help users understand and analyze the internal relationship between data. From the perspective of chart implementation and construction, the construction of a visual chart is divided into the following levels.

1. Data layer. Data layer is the foundation of visualization and includes the process of collecting, organizing and preparing data. At this level, more attention will be paid to the authenticity, accuracy and completeness of the data. In practice, it is common to use a separate backend for data collection and preprocessing.
2. Graph mapping layer. This layer is concerned with mapping data to visual elements. At this level, developers choose appropriate forms of visualization to effectively communicate the relationships and trends of the data.
3. Interaction layer, this layer is concerned with the process of interaction between the chart and the user, including mouse operation, keyboard input, and so on. This layer is coupled with the graph-mapping layer, providing an interactive way for users to explore the data in greater depth and see more detailed information.
4. Annotation and explanation layer, this layer provides the necessary text and images to understand the diagram, including explanatory text, legend, comment, etc.



### 3. Backend Implementation

#### 3.1 Profiler Overview

Python program running data acquisition is an important part of this project. This project implements a total of two kinds of profilers, corresponding to deterministic performance analysis and statistical performance analysis.

`cProfile` is a built-in performance analysis module for Python and is part of the Python standard library written in C.  It is a wrapper for the `profile` module, which records the number and name of functions executed during the program execution, the total time and number of times each function is executed, and imports the above information into the `.prof` format file.

There are two ways to accurately record the start and duration of a function call. One way is to set up a hook function on the context switch during the function execution. Each time the function is called (the context switch), the current stack frame is logged, thus obtaining the entire call stack. Another approach is to set a timer during the execution of the function, sample the run of the function at a fixed interval, record the run information of the time slice, and finally restore the entire call stack through all of the above slices. It is important to note that accurate performance analysis always affects performance (even greatly), so the goal in this process is to minimize distortion rather than eliminate it.

`sys.setprofile` is a function in Python's built-in `sys` module that sets up a global profiling function that allows tracking and profiling of events at the Python interpreter level. While setting up an profiling function with `sys.setprofile` , the function will be called every time a specific Python event (such as a function call, function return, exception, etc.) occurs.When the profiling function is called, three parameters are passed: frame , event, and arg, representing the current stack frame, the event type, and the event's parameters, respectively. So all the data can be retrieved that the function is running.

Corresponding to the two performance analysis methods, using `cProfile` profiler has less impact on performance, but only partial information can be recorded. Using `sys.setprofile` records every details of each call, but it has poor performance because it requires extra Python code to be executed for each context switch.

#### 3.2 Call Stack Recovery

To avoid the profiler having too much negative impact on performance, the profiler usually collects as little information as possible only at runtime. In this case, the profiler only collects the start and duration of each function call, so we need to devise an algorithm to retrieve the call stack from this information. Given that only one function can be executed at a time, the following algorithm is proposed and implemented.

```pseudocode
def recover_stack(raw_stack_events):
	initialize min heap minh based on invocation time of each event
	initialize max heap maxh base on finish time of each event
	initialize stack stk
	level = 0
	
	while size(minh) != 0:
		temp_event = minh.pop()
		temp_event.level = level
		level += 1
		if size(stk) != 0:
			// append temp_event to stk's first event
			stk[-1].children.append(temp_event)
		stk.append(temp_event)
		
		while maxh.peak() == temp_event:
			maxh.pop()
			stk.pop()
			if size(stk) == 0:
				break
			level -= 1
			temp_event = stk[-1]
	return stk
```

==Pseudocode 3-2==

The restored call stack has the same structure as the call stack itself. We can see from the Json file that each event object has an additional children property that represents the function events directly called in that event.

#### 3.3 Encapsulation of Profiler

In this project, in order to facilitate the call of the analyzer and improve the system integration, all the functions of the analyzer are integrated in a Python package. The structure of a Python package is shown below.

```file
.
├── __init__.py
├── __main__.py
├── frame.py
├── runner.py
├── profiler
│   ├── __init__.py
│   ├── base_profiler.py
│   ├── event_profiler.py
│   └── model
│       ├── __init__.py
│       ├── base_model.py
│       └── calc_model.py
└── view
    ├── __init__.py
    ├── list_view.py
    └── stack_view.py
```

==Figure 3-3-1==

1. In the `profiler`, `base_profiler` wraps the profiler based on the `cProfile` implementation and `event_profiler` wraps the profiler based on the `sys.setprofile` implementation. `model` stores the objects and classes used by profilers.
2. `frame` and `view` define interfaces that are exposed to the user, receive user arguments and return processed results. Used to further encapsulate the profiler.
3. `runner` implements a code runner to invoke new threads to execute code, creating an isolated runtime environment.

#### 3.4 Server and Cluster

In order to realize the function of frontend uploading code and backend running and returning data, this project implements a backend server cluster system. The system has the following features: allows users to customize the execution environment of Python code, including the main version of Python, the version of dependencies. The execution data for each environment is returned after the runtime data is obtained. Its architecture diagram is shown below.

==Figure 3-3-3==

==Figure 3-3-2==

In order to implement the above functions, this project uses Docker to implement two processing nodes: master and slave. In a normal request flow, the master node receives the code uploaded by the user and the special environment configuration, and attaches a global id to the input, which is easy to store and track. After receiving the uploaded code, the master node will backup the code file locally and forward it to the slave node, which will not run directly after receiving the code file but wait for the master node to process it. The slave node will use conda to create a virtual environment for each run, run the profiler described above, and return the results via the message queue.

The above architecture ensures the independence, robustness, security and reliability of the whole system. Using docker to isolate the master and slave nodes makes the whole system structure and division of labor clear, and ensures that the analysis process will not be interfered by other factors. Using the conda virtual execution environment not only ensures the purity of the runtime environment, but also avoids the dependency problem between python libraries as much as possible, making it possible to use the runtime comparison between different versions of the library.



### 4. UI Implementation

#### 4.1 Technological Stack Overview

There are many browsers and javascrip-based stacks and ecosystems out there. Next I will analyze the development requirements of the frontend interface in this project and choose the right tool chain.

The frontend interface of the visualization project contains charts and other sections. React was chosen as the main framework for this project because it implements a set of virtual DOM and update rules to better manage state and improve performance. JSX helps developers organize their code better. On top of that, due to the complex relationship between the different components of the visual interface, a more efficient way to centrally manage state and events was needed, so the project introduced Redux as a global state manager. In order to better manage style sheets, this project introduces LESS. To facilitate faster packaging and development, this project uses Vite instead of the official scaffolding to initialize the project. The above tool chain provides an efficient way to create the project, which provides a good foundation for the following visualization charts and interactions.

|   Library    |                Function                 |
| :----------: | :-------------------------------------: |
|    React     |   Overall framework, state management   |
|     Vite     | Project packaging and plugin management |
|     LESS     |         Style sheet management          |
| react-router |  Router implementation and management   |
|    Redux     |         Global state management         |
|    D3.js     |   Visualization chart implementation    |
|     antd     |       Frontend component library        |
|    ahooks    |       Hook function logic library       |

#### 4.2 Combination with React and Other Components

The main problem in the combination of D3.js and React framework is that D3.js requires complete control of DOM nodes. However, the design philosophy of React framework is to avoid direct operation of DOM as much as possible, so there are incompatibility between the two design concepts.

Although React has JSX syntax and seems to embed HTML similar DOM tree in JS, these are all virtual DOM nodes. Different from the DOM in actual rendering, D3.js cannot operate these virtual DOM directly. Therefore, the general solution is to give a DOM node reference (ref) in the React component, giving D3.js full control of the internal parts of the node, and the rest is handled by React. However, the latest version of React recommends writing based on hooks and functional components instead of a strict lifecycle approach. The solution is to simulate lifecycle using hook function. In particular, the `useEffect` dependency list is emptied, which guarantees that the DOM will only be initialized once, and then store the nodes that need to be activated as the data changes in the component's state. Then write another `useEffect` hook that is triggered by data updates, using only the saved states to manipulate the action effect.

The foundamental idea is the same for the combination between customized components and React, one problem is that event listener should be manully managed properly, in order to solve this problem, `useLayoutEffect` hook function is used. `useLayoutEffect` is a hook in React that is similar to `useEffect`, but it fires synchronously after all DOM mutations. This makes it useful for reading layout from the DOM and re-rendering, ensuring that updates occur before the browser has a chance to paint. It's the counterpart to `useEffect` for cases where you need to make changes to the DOM and want to ensure they're applied before the user sees the updated UI. in this senerio `useLayoutEffect` is used to make sure event listener is added right before the UI is painted.

#### 4.3 Chart Drawing

In this project, a variety of charts, including icicle graph, cascade treemap, bar chart, and so on ne ed to be drawn and a variety of custom interactions to these charts need to be added. Chapter 2-2 introduces many visualization frameworks. Among these frameworks, this project chooses D3.js as the main framework of the project, because compared with ECharts, HighCharts and other chart libraries, D3.js is equivalent to centralized management of tools such as SVG, only concerned with the drawing results of the image. No preset image combination logic. It's a lot harder to develop, but it's a lot closer to the bottom, making it easier to customize the chart.





#### 4.4 React and Global State Management

Global state management tools in React are designed to manage and centralize the application state, making it easier to handle complex interactions across multiple components. One widely used tool for this purpose is Redux.

The reason why this project use global state management is because components has to communicate with each other across the whole app. Consider the following scenario: When a user selects an element from one chart, information about that element node is displayed on another main board, and another chart changes dynamically. Passing values directly between different components through parent and child components and manually managing rendering through state would be too complex and add a lot of redundancy, with redux, individual components are free to re-render as needed.

#### 4.5 Layout Control

Layout control in React projects is essential for creating user-friendly, performant, and maintainable web applications. It impacts user experience by ensuring intuitive navigation and responsive design across devices. Optimizing component arrangement boosts performance by improving load times and reducing rendering issues. Good layout practices enhance maintainability and scalability, making it easier to update the application and add new features. 

But the layout control is very difficult, because it is related to both the content presented and the logic of the program, so it is difficult to decouple the content and the layout. The following principles were followed for layout control in this project.

First of all, this project fully uses flex layout control, which allows the application to be displayed on computers as well as mobile phones and tablets, because the layout control system automatically detects the screen size and arranges the appropriate size and position for each component.

The application is divided into several parts: header, sider, main content and footer. Each part of the application is completely independent, and each part manages its child components separately, resulting in a top-down structure for layout control and data flow. 

In general, I get the size and position information of the component through a combination of useState and useLayoutEffect. By defining the size of the component in jsx, the component will have a chance to get its own size information before being rendered to the screen. This information is then saved to the state using the useLayoutEffect hook for later use.

It's worth noting that when a component has two levels of child components, this way of getting the position and size of the component by itself doesn't work. Since the rendering of the child component is always completed before the rendering of the parent component, and the position and size information of the child component is inherited from the parent component, if we continue in the previous way, the child's size will be set to a default size, usually 0 for both length and width. This is where you need to pass layout information from parent to child via props.

#### 4.6 Performance Optimization

Graphical interface is a complex system, the first rendering, user actions and window size changes will lead to changes in the displayed content. It would be expensive to redraw all components and compute all data for every change, resulting in unacceptable stalling. I optimized the component generation and retention logic, and through the fine-grained control of the components, the performance of the program reached a satisfactory level.

First of all, almost all presentation depends on layout information, which is why layout information is so important. After obtaining the layout information through the above method, some important information will be calculated, including the size of the child components, the axis of the graph, and the position relationship information of the graph. These contents are highly bound to the layout information. If the layout does not change, even if the image is redrawn, these contents will not change. This leads to a substantial reduction in the amount of recomputation required after each interaction. 

These fuctions are achieved by `useMemo` and `useCallback` hooks, `useMemo` and `useCallback` are React hooks that optimize performance by memoizing values and functions, respectively. `useMemo` recalculates a value only when its dependencies change, preventing expensive operations on every render. `useCallback` returns a memoized version of a function, ensuring it only changes if its dependencies do, which helps avoid unnecessary re-renders of child components. Both hooks are key for improving efficiency in React applications by reducing computation and rendering work.



### 5. Case Study, Discussion and Conclusion

#### 5.1 Interview

In the project, we distributed the system to undergraduate students of science and engineering universities for evaluation, including students of computer science and engineering, other science and engineering students with programming experience, and zero-foundation students who are currently learning Python programming. During the evaluation process, invited students can freely use the system and compare different systems using the same example. The vast majority of the invited people gave very positive evaluation of the system, and they fully affirmed the advantages of the system. Some students from the Department of Computer Science and Engineering gave the following comments: "This system combines deterministic and statistical performance analysis, so I can get a very good idea of how precise sampling is affecting performance." "The comparison between different versions of the library is very useful, it helps me understand the impact of different versions on specific programs." Other students were also satisfied with the ease of use of the system and felt that using Docker brought a number of benefits to the system, including near zero cost deployment and out-of-the-box features. We also received suggestions from students on the improvement of the system, including suggestions on the aesthetic degree of the system itself and suggestions on the interactive logic of the frontend interface of the system.

#### 5.2 Limitation and Future Work





---

ABS

随着软件工程领域的发展软件的规模急剧扩大, 开发者更加迫切地需要深入了解程序运行时的运行细节. 软件运行过程中会产生大量信息, 包含函数调用信息, 运行时间信息以及其它统计性质的信息. 通过可视化的手段我们可以将以上信息用更生动的方式展现出来. 提供更快速的理解程序运行的方式.

本项目实现了基于cProfile的统计性能分析分析器和基于sys.setprofile的确定性性能分析分析器, 将以上分析器使用python package进行封装. 在获取程序性能数据后使用浏览器工具链对得到的数据进行可视化, 与现有的可视化工具相比, 本工具拥有更加丰富的功能, 交互性更强的界面, 并且结合了两种性能分析方式使得结果可信度更高.

1.1

作为软件分析的重要手段, 性能分析拥有非常重要的地位. 性能分析包括确定性性能分析和统计性能分析, 其中确定性性能分析涉及代码级性能分析和系统级监控, 能精确记录函数执行过程中的所有细节, 但是由于监控事件需要消耗的大量资源, 确定性性能分析可能会造成性能为题. 统计性能分析通过抽样和概率方法尝试对代码进行分析, 由于其较小的性能开销和较广的适用范围, 被广泛运用于生产环境下的监控. 2024年一项有关xz utils的安全漏洞被爆出. xz utils 5.6.0 和 5.6.1 版本被植入后门, 涉及包括Debian, Fedora等多个Linux稳定发行版本. 幸运的是, 微软的安全研究员Andres Freund通过性能分析工具发现了运行时的异常并上报了可疑情况, 这个事件充分说明了性能分析工具的必要性.

1.2

性能分析会产生大量数据, 这些数据天然适合用来进行可视化分析. 利用图形和图表将性能数据呈现出来可以使得开发者和分析人员更加直观地理解软件的性能表现. 函数运行产生的调用栈可以与时间轴结合画出精确的调用细节, 其它运行数据同样可以通过统计和数据挖掘获取有用的信息并通过图表的方式展示.

目前市面上有针对不同平台, 不同编程语言的各种可视化性能分析工具. 其中, 既有工业级的系统监控完整解决方案, 自动化测试与监控平台, 又有面向个人开发者的小型工具. 下文会简单介绍各种面向个人开发者的小型性能分析工具, 对它们的功能进行分析, 并确定本项目的目标.

1.   vprof

     vprof是一个为Python提供的可视化性能分析工具. 它内置一个web服务器, 通过命令行调用, 可以直观地展示CPU占用, 内存使用和代码内部运行数据.

2.   gprof2dot

     gprof2dot是一个使用Python实现的开源脚本, 它用于将多种性能分析工具生成的性能数据转化为可视化调用图, 调用图中的节点为有向无环图的结构, 并且可以显示函数之间的调用关系和各个函数在执行中的时间占比.

3.   JetBrain Profiler

     JetBrain IDE为开发者提供了方便的性能分析工具. 这些工具能帮助开发者在开发过程中方便地对程序中的CPU与内存使用率进行检测, 并且提供了图表方便开发者查看函数间的调用关系.

1.4

本项目以Python确定性能分析为入手点, 对可视化系统的底层实现与优化, 分析器的实现及封装等内容进行研究, 完成了一个全流程的Python性能可视分析系统, 具体各章节的内容如下所示.

第一章为绪论部分, 主要介绍了研究背景, 研究意义, 国内外研究相关工作与论文的整体架构.

第二章为前端可视化技术研究, 主要介绍当前以浏览器为基础的主流绘图技术, 前端常用可视化库的对比以及其它库的介绍, 针对本项目性能数据的可视化选择合适的技术方案, 并对可视化的整体流程加以叙述.

第三章为分析器的实现与封装, 主要介绍了分析器的功能与整体架构以及RESTFul服务端的实现,

第四章对可视化系统前端进行了细致的介绍, 从类库的结合使用, 界面性能优化, 布局控制, 自定义组件的实现方面对系统进行了细致的介绍.



2.1

可视化的基本流程如图2-1所示

1.   用户通过界面选定需要进行性能分析的python源文件并上传
2.   RESTFul 服务器接收前端上传的文件, 调用分析器模块
3.   分析器模块运行代码, 分析并整合运行数据
4.   通过 RESTFul 服务器向前端传回处理后的数据
5.   前端接收数据并绘制可视化界面

2.2 

浏览器技术是现代互联网和数字通信的基石之一, 伴随着浏览器技术的发展, 各种规范和工具链趋于成熟. 作为可视化技术的载体, 图表的重要性不言自明. 在早期, 在浏览器绘图通常依赖于 Flash, Silverlight 等插件. 由于安全性, 兼容性和性能方面的优势, 以上工具链逐渐被 HTML 工具链取代. 目前浏览器端主流的绘图技术有 SVG, Canvas, WebGL 等

SVG (Scalable Vector Graph) 在2003年成为 W3C 的推荐标准, SVG 绘制的图形是矢量图形, 在缩放时可以保证不失真, 可以用于高精度图形绘制. SVG 的兼容性良好, 完全支持 DOM. HTML5 Canvas 也是 W3C 的标准之一, 与 SVG 的标准不同, Canvas 的绘图基于像素, 在缩放时会导致锯齿. Canvas 的绘图效率很高, 在数据量大, 图像零碎繁重的绘图时有更好的表现.

可视化图表包含大量的基础图形. 作为底层的浏览器绘图技术, SVG 与 Canvas 通常不直接用于可视化图表的绘制, 此项任务通常用各类可视化框架来完成. 可视化框架是对 SVG, Canvas 的封装, 不同与SVG与Canvas是从绘制单独的图形出发, 各类可视化框架从所展示的数据, 图形间的联系, 图像的意义角度出发, 提供了各种层次的逻辑/交互/信息展示的封装, 从底层实现来分类, D3, Highcharts, antv.X6 等基于 SVG 实现, ECharts 等基于 Canvas 实现.

2.3

作为直观的展示数据的方式, 可视图表帮助用户理解和分析数据之间的内在联系, 从图表的实现和构建角度, 一个可视化图表的构建分为以下几个层次.

1.   数据层, 数据层是可视化的基础, 包括收集, 整理和准备数据的过程. 在这个层次会更加关注数据的真实性, 准确性和完整性. 在实践操作中, 通常使用一个独立的后端用于数据的收集和预处理.
2.   图形映射层, 这一层关注数据映射到可视化元素的过程. 在这个层次上, 开发者选择适当的可视化形式来有效地传达数据的关系和趋势.
3.   交互层, 这一层关注图表与用户交互的过程, 包括鼠标操作, 键盘输入等等多种形式. 这个层次与图形映射层相互耦合, 提供交互性的方式让用户更深入地探索数据, 查看更详细的信息.
4.   标注与解释层, 这一层提供了理解图表所必须的文字, 图像, 包括说明性文字, 图例, 注释等.



3.1

Python程序运行数据获取是本项目的一项重要内容, 本项目一共实现了两种profiler, 分别对应确定性性能分析和统计性能分析.

`cProfile` is a built-in performance analysis module for Python and is part of the Python standard library written in C. 它是对 `profile` 模块对封装, 能够记录程序执行过程中所执行的函数个数及名称, 各个执行函数的总计时间和次数, 并将以上信息导入 `.prof` 格式文件. 

 `cProfile` 具有优异的性能, 但是无法记录每次函数调用的精确开始时间和持续时间, 因此需要自己实现用于确定性性能分析的分析器. 

目前有两种方式来实现对函数调用开始和持续时间的精确记录, 一种方法是通过在函数执行过程中上下文切换时设置钩子函数, 每一次函数的调用(上下文切换)时都会记录当前栈帧的信息, 从而获取整个调用栈. 另一种方法是在函数执行过程中设置一个定时器, 固定时间间隔对函数运行进行采样, 记录该时间切片的运行信息, 最后通过以上所有切片还原出整个调用栈. 需要注意的是, 精确的性能分析总是会影响性能(甚至影响巨大), 所以在这个过程中我们的目标是尽量减小失真而不是消除失真,

sys.setprofile...

与两种性能分析方式相对应, 使用 cProfile 分析器对性能影响较小, 但是只能记录部分信息. 使用 sys.setprofile 虽然能记录每一次调用的细节, 但是性能较差, 因为每一次上下文切换都要执行额外的python代码

3.2

为了避免分析器对性能产生过多的负面影响, 分析器通常只在运行时收集尽可能少的信息. 在这里, 分析器只收集了每个函数调用的开始和持续时间, 因此需要设计出一个算法通过以上信息还原出调用栈. 已知在同一时间只能执行一个函数, 以下的算法被提出和实现.

还原后的调用栈拥有调用栈本身的结构, 从json文件来看, 每个事件对象都有一个额外的children属性, 用来表示该事件中直接调用的函数事件. 

3.3

在本项目中, 为了方便分析器的调用和提高系统集成度, 所有分析器的功能均被集成在一个 Python package中. Python package的结构如下所示

1.   在 `profiler` 中, `base_profiler` 封装了基于 `cProfile` 实现的分析器, `event_profiler` 封装了基于 `sys.setprofile` 实现的分析器. 
2.   `frame` 与 `view` 定义了暴露给用户的接口, 接收用户的参数并返回处理后的结果. 用于将profiler进一步封装.
3.   `runner` 中实现了一个代码运行器, 用于唤起新的线程执行代码, 创建一个隔离的运行环境.

3.4

为了实现前端上传代码, 后端运行并返回数据的功能, 本项目实现了一个后端服务器集群系统. 该系统拥有以下功能: 允许用户自定义Python代码的执行环境, 包括Python的主要版本, 依赖项的版本. 在得到运行运行数据后会返回各个环境的执行数据. 其架构图如下所示

为了实现以上功能, 本项目使用docker实现了两个处理节点: master与slave. 在通常的请求处理流程中, master节点接收用户上传的代码与特殊环境配置, 并为以上输入附带一个全局id, 方便进行数据的储存和追踪. 收到上传的代码后, master节点会在本地备份代码文件后将其转发给slave节点. slave节点在收到代码文件后并不会直接运行, 而会等待master节点处理, 通过消息队列接收master发送的更改运行环境的请求. slave节点会使用conda为每一次运行创建虚拟环境, 运行上文中阐述的profiler, 并通过消息队列返回结果. 

以上架构保证了整个系统的独立性, 鲁棒性, 安全性和可靠性. 使用docker隔离master和slave节点使得整个系统结构和分工明确, 确保分析过程不会受其它因素干扰. 使用conda虚拟执行环境既保证了运行环境的纯净性, 又尽可能地避免了python库之间的依赖问题, 使得使用不同版本库之间的运行对比成为可能.



4.1

现在市面上有很多基于浏览器与Javascript语言的技术栈与技术生态. 接下来我将分析本项目中前端界面的开发需求并以此选择合适的工具链.

可视化项目的前端界面包含图表和其他部分. 本项目选择React作为项目的主要框架, 因为它实现了一套虚拟浏览器节点与更新规则来更好地管理状态与提高性能. JSX能帮助开发者更好地组织代码. 在此之上, 由于可视化界面不同组件之间的联系非常复杂, 需要一种更高效的方式集中化管理状态以及事件, 因此本项目引入了redux作为全局状态管理. 为了更好地管理样式表, 本项目引入了less. 为了方便更快地打包和开发, 本项目使用vite代替官方脚手架初始化项目, 以上工具链提供了高效的方式创建项目, 为接下来的可视化图表与交互提供了良好的基础.

4.3

本项目需要实现各种样式图表的绘制, 包括icicle graph, cascade treemap, bar chart等等各式图表并为这些图表添加种类各异的自定义交互方式. 第二章介绍了很多可视化框架, 在这些框架中本项目选择 D3 作为项目的主要框架, 因为相比于 ECharts, HighCharts 等图表库, D3 在功能上相当于集中管理 SVG 等工具, 只关心图像的绘制结果, 不预设图像组合的逻辑. 虽然开发起来难度大很多但更接近底层, 方便对图表进行自定义. 



5.1

与其它开源项目相比, 本系统

在项目中我们将系统发放给理工科大学本科学生邀请其作出评价, 被邀请的学生包括计算机科学工程系学生, 有编程经验的其它理工科学生, 正在学习Python编程的零基础学生等. 在评价过程中被邀请学生可以自由使用系统, 使用同一个样例在不同系统之间进行对比. 绝大多数被邀请人给予系统非常正面的评价, 他们对系统的优点进行了充分的肯定. 部分计算机科学与工程系的同学给予了如下评价: “这个系统将确定性性能分析和统计性能分析结合在一起, 我能非常清楚地了解到精确采样究竟对性能造成了何种程度的影响. ” “不同版本库之间的对比非常实用, 它很好地帮助我了解了不同版本对具体程序的影响.” 其它同学同样对系统的易用性表示满意, 他们认为使用docker为系统带来了大量的好处, 包括近乎零成本的部署与开箱即用的特性. 我们同样收到了同学对系统的改进建议, 包括对系统本身美观程度的建议, 系统前端界面交互逻辑的建议.

5.2

项目的





---



